{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-17T05:43:02.696400Z","iopub.execute_input":"2022-01-17T05:43:02.696802Z","iopub.status.idle":"2022-01-17T05:43:02.703471Z","shell.execute_reply.started":"2022-01-17T05:43:02.696734Z","shell.execute_reply":"2022-01-17T05:43:02.702570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# lets import some stuff\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.utils.np_utils import to_categorical\nimport re\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"08c3425b67173c7162a586c06cbb279f29f946a9","execution":{"iopub.status.busy":"2022-01-17T05:43:05.683639Z","iopub.execute_input":"2022-01-17T05:43:05.684299Z","iopub.status.idle":"2022-01-17T05:43:06.722369Z","shell.execute_reply.started":"2022-01-17T05:43:05.684248Z","shell.execute_reply":"2022-01-17T05:43:06.721434Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', delimiter='\\t')\ndf = df[['Phrase', 'Sentiment']]\n\npd.set_option('display.max_colwidth', -1)\ndf.head(3)","metadata":{"_uuid":"0e0fe2cee592ee4313e67b342fffdd9bb851578d","execution":{"iopub.status.busy":"2022-01-17T05:43:10.062461Z","iopub.execute_input":"2022-01-17T05:43:10.063220Z","iopub.status.idle":"2022-01-17T05:43:10.508376Z","shell.execute_reply.started":"2022-01-17T05:43:10.063146Z","shell.execute_reply":"2022-01-17T05:43:10.506988Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"So, you can see that the `Phrase` data which we're going to train our model against hasn't been cleaned so we need to do it.\n\nI am just going to do something very simple, which is to turn url's into `url`, remove anything that's not alphanumeric or a space. Then lowercase what's left.","metadata":{"_uuid":"0df307c3d8d0ea1bd773064e9eb1ba30ced73848"}},{"cell_type":"code","source":"def clean_str(in_str):\n    in_str = str(in_str)\n    # replace urls with 'url'\n    in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n    in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n    return in_str.strip().lower()\n\n\ndf['text'] = df['Phrase'].apply(clean_str)","metadata":{"_uuid":"1560d4c17410b19f0d420f4afb074e0721b15e56","execution":{"iopub.status.busy":"2022-01-17T05:43:20.204138Z","iopub.execute_input":"2022-01-17T05:43:20.204482Z","iopub.status.idle":"2022-01-17T05:43:22.354738Z","shell.execute_reply.started":"2022-01-17T05:43:20.204435Z","shell.execute_reply":"2022-01-17T05:43:22.353547Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Our data is classified into 5 different classes, very negative, slightly negative, neutral, slightly positive and very positive.\n\nSadly our dataset isn't balanced, so we need to do that ourselves","metadata":{"_uuid":"ec14d0e2bb4fcfcee507442015c643ae3df76e46","trusted":true}},{"cell_type":"code","source":"df.Sentiment.value_counts()","metadata":{"_uuid":"0a42e34a57fa9e55a4a613ce60d7b5f6314a6faf","execution":{"iopub.status.busy":"2022-01-17T05:43:25.682919Z","iopub.execute_input":"2022-01-17T05:43:25.683322Z","iopub.status.idle":"2022-01-17T05:43:25.694310Z","shell.execute_reply.started":"2022-01-17T05:43:25.683261Z","shell.execute_reply":"2022-01-17T05:43:25.693146Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_0 = df[df['Sentiment'] == 0].sample(frac=1)\ndf_1 = df[df['Sentiment'] == 1].sample(frac=1)\ndf_2 = df[df['Sentiment'] == 2].sample(frac=1)\ndf_3 = df[df['Sentiment'] == 3].sample(frac=1)\ndf_4 = df[df['Sentiment'] == 4].sample(frac=1)\n\n# we want a balanced set for training against - there are 7072 `0` examples\nsample_size = 7072\n\ndata = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)","metadata":{"_uuid":"ecbc5ac7b4d7195cf65050b557ca0d9a383032b4","execution":{"iopub.status.busy":"2022-01-17T05:43:28.276736Z","iopub.execute_input":"2022-01-17T05:43:28.277308Z","iopub.status.idle":"2022-01-17T05:43:28.376653Z","shell.execute_reply.started":"2022-01-17T05:43:28.277257Z","shell.execute_reply":"2022-01-17T05:43:28.375557Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to tokenize our sentences into vectors. Before we do that we need to pick a vector size.","metadata":{"_uuid":"25e8946dae515f87a21959d4387566de945d2154"}},{"cell_type":"code","source":"data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))\nprint(\"mean length of sentence: \" + str(data.l.mean()))\nprint(\"max length of sentence: \" + str(data.l.max()))\nprint(\"std dev length of sentence: \" + str(data.l.std()))","metadata":{"_uuid":"ec44d5d08736cf56b40718228427a50f7355f1f1","execution":{"iopub.status.busy":"2022-01-17T05:43:31.245155Z","iopub.execute_input":"2022-01-17T05:43:31.245592Z","iopub.status.idle":"2022-01-17T05:43:31.337486Z","shell.execute_reply.started":"2022-01-17T05:43:31.245526Z","shell.execute_reply":"2022-01-17T05:43:31.335009Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# these sentences aren't that long so we may as well use the whole string\nsequence_length = 52","metadata":{"_uuid":"0c546ced21a6fc3e8ffb9747e192e7da364911a2","execution":{"iopub.status.busy":"2022-01-17T05:43:34.048270Z","iopub.execute_input":"2022-01-17T05:43:34.048633Z","iopub.status.idle":"2022-01-17T05:43:34.053340Z","shell.execute_reply.started":"2022-01-17T05:43:34.048581Z","shell.execute_reply":"2022-01-17T05:43:34.052222Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"max_features = 20000 # this is the number of words we care about\n\ntokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\ntokenizer.fit_on_texts(data['Phrase'].values)\n\n# this takes our sentences and replaces each word with an integer\nX = tokenizer.texts_to_sequences(data['Phrase'].values)\n\n# we then pad the sequences so they're all the same length (sequence_length)\nX = pad_sequences(X, sequence_length)\n\ny = pd.get_dummies(data['Sentiment']).values\n\n# where there isn't a test set, Kim keeps back 10% of the data for testing, I'm going to do the same since we have an ok amount to play with\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nprint(\"test set size \" + str(len(X_test)))","metadata":{"_uuid":"e7d26ca92f2f3bf193df765fa1621d85bdf22297","execution":{"iopub.status.busy":"2022-01-17T05:43:36.897806Z","iopub.execute_input":"2022-01-17T05:43:36.898186Z","iopub.status.idle":"2022-01-17T05:43:38.733062Z","shell.execute_reply.started":"2022-01-17T05:43:36.898124Z","shell.execute_reply":"2022-01-17T05:43:38.732050Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Model 1: Random embeddings\n\nLets build our model. In general I'm going to just use the same hyperparameters as Kim does (see section 3.1 of his paper) apart from the embedding dimension\n\nKeras has an Embedding layer we can use here. If you don't specify a custom way to embed text (something we will do later with w2v) Keras will do it randomly with a normal (Gaussian) distribution for you","metadata":{"_uuid":"461ca35a112a721e3c739c3aa82599db6e36c499"}},{"cell_type":"code","source":"embedding_dim = 200 # Kim uses 300 here\nnum_filters = 100\n\ninputs = Input(shape=(sequence_length,), dtype='int32')\n\n# use a random embedding for the text\nembedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n\nreshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n\n# Note the relu activation which Kim specifically mentions\n# He also uses an l2 constraint of 3\n# Also, note that the convolution window acts on the whole 200 dimensions - that's important\nconv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n\n# perform max pooling on each of the convoluations\nmaxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\n# concat and flatten\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\n\n# do dropout and predict\ndropout = Dropout(0.5)(flatten)\noutput = Dense(units=5, activation='softmax')(dropout)","metadata":{"_uuid":"6efb6f8c0974ae0a5b12c4374ac9b4b9b0f217ac","execution":{"iopub.status.busy":"2022-01-17T05:43:44.401228Z","iopub.execute_input":"2022-01-17T05:43:44.401587Z","iopub.status.idle":"2022-01-17T05:43:44.624616Z","shell.execute_reply.started":"2022-01-17T05:43:44.401537Z","shell.execute_reply":"2022-01-17T05:43:44.623737Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Kim uses Adadelta as his optimizer. After some experimentation it's clear you get the same results with adam, it just happens to get there MUCH slower with adadelta (which makes sense). For everyone's sanity I'm going to use adam here - if you'd like to follow Kim's paper closer than me then all you have to do is swap `adam` for `adadelta` in the next cell and increase the number of epochs to around 100","metadata":{"_uuid":"747582d6ec9c133dd719ce8ed9017b8baa870c56"}},{"cell_type":"code","source":"model = Model(inputs=inputs, outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"_uuid":"264a64231f955b32197618dc83c26a907635bc1c","execution":{"iopub.status.busy":"2022-01-17T05:43:48.203161Z","iopub.execute_input":"2022-01-17T05:43:48.203528Z","iopub.status.idle":"2022-01-17T05:43:48.275429Z","shell.execute_reply.started":"2022-01-17T05:43:48.203470Z","shell.execute_reply":"2022-01-17T05:43:48.274332Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 32 # Kim uses 50 here, I have a slightly smaller sample size than num\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)","metadata":{"_uuid":"cb108fefc0af9f24fa738f7d67713848cb626b31","execution":{"iopub.status.busy":"2022-01-17T05:43:51.956424Z","iopub.execute_input":"2022-01-17T05:43:51.956790Z","iopub.status.idle":"2022-01-17T06:38:53.896152Z","shell.execute_reply.started":"2022-01-17T05:43:51.956745Z","shell.execute_reply":"2022-01-17T06:38:53.895075Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the training","metadata":{"_uuid":"51336f27d423d15777d68a3cae509adab84aa3a9"}},{"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"cd8c6247c101d7e343c7b6f21239ea558cd16429","execution":{"iopub.status.busy":"2022-01-17T06:41:18.286395Z","iopub.execute_input":"2022-01-17T06:41:18.286777Z","iopub.status.idle":"2022-01-17T06:41:18.792616Z","shell.execute_reply.started":"2022-01-17T06:41:18.286717Z","shell.execute_reply":"2022-01-17T06:41:18.791524Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"as you can see this is a bit noisey, but it doesn't overfit too badly or do crazy things, so I'm happy!\n\nSo how good is this model? Lets test it with our test set","metadata":{"_uuid":"054cd8161230abe3c31ceb0eb53b369b11562d91"}},{"cell_type":"code","source":"y_hat = model.predict(X_test)","metadata":{"_uuid":"9d050fea58cf2e351274435b3c9faa9d9ae99854","execution":{"iopub.status.busy":"2022-01-17T06:41:22.546042Z","iopub.execute_input":"2022-01-17T06:41:22.546373Z","iopub.status.idle":"2022-01-17T06:41:24.439707Z","shell.execute_reply.started":"2022-01-17T06:41:22.546336Z","shell.execute_reply":"2022-01-17T06:41:24.438514Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))","metadata":{"_uuid":"d08a240de0b06b5a3042be125a1895c593135a4e","execution":{"iopub.status.busy":"2022-01-17T06:41:26.045017Z","iopub.execute_input":"2022-01-17T06:41:26.045412Z","iopub.status.idle":"2022-01-17T06:41:26.077857Z","shell.execute_reply.started":"2022-01-17T06:41:26.045347Z","shell.execute_reply":"2022-01-17T06:41:26.076837Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))","metadata":{"_uuid":"adcaa3b5972bda1b93cc270c6f50de66eb8708c9","execution":{"iopub.status.busy":"2022-01-17T06:41:29.450120Z","iopub.execute_input":"2022-01-17T06:41:29.450485Z","iopub.status.idle":"2022-01-17T06:41:29.489033Z","shell.execute_reply.started":"2022-01-17T06:41:29.450427Z","shell.execute_reply":"2022-01-17T06:41:29.488060Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"This is suprisingly good - a random model would achieve a score of ~0.2 (there are 5 classes)","metadata":{"_uuid":"ae5504b1efeadc3ccf247e88a5b6468540bf05bd"}},{"cell_type":"markdown","source":"# Model 2: Static word2vec\n\nNow rather than randomly assign vectors we're going use w2v embeddings. This took me quite a long time to get right, so I'll walk through it line by line\n\nFirstly, we need to load the model. As mentioned I couldn't find the model Kim used, but Kaggle does have a great 200D model trained using GloVe.","metadata":{"_uuid":"c34ecf0298d9a96625397833231ff872723e96f3"}},{"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join('../input/glove-global-vectors-for-word-representation', 'glove.6B.200d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"_uuid":"ce8803408785f0222cdb4d424c574135fe0b8d2e","execution":{"iopub.status.busy":"2022-01-17T06:41:33.325143Z","iopub.execute_input":"2022-01-17T06:41:33.326309Z","iopub.status.idle":"2022-01-17T06:42:08.205400Z","shell.execute_reply.started":"2022-01-17T06:41:33.326232Z","shell.execute_reply":"2022-01-17T06:42:08.204254Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"_uuid":"573d5e1e5b170a77f61ab66990ba4cb6bc8b66a4","execution":{"iopub.status.busy":"2022-01-17T06:42:15.515538Z","iopub.execute_input":"2022-01-17T06:42:15.516068Z","iopub.status.idle":"2022-01-17T06:42:15.522573Z","shell.execute_reply.started":"2022-01-17T06:42:15.516000Z","shell.execute_reply":"2022-01-17T06:42:15.521059Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Now for the fun bit, we run through all the words in our tokenizer and look for them in the w2v model. If they exist we add them to our embedding matrix (ie our embeddings) if they don't then we assign a random vector","metadata":{"_uuid":"a28233126e7ce05bde9cf1c6fdbff62f5bd62d66"}},{"cell_type":"code","source":"num_words = min(max_features, len(word_index)) + 1\nprint(num_words)\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector\n    else:\n        # doesn't exist, assign a random vector\n        embedding_matrix[i] = np.random.randn(embedding_dim)","metadata":{"_uuid":"6ab06dd855b04f70166527630ffdbe72b64d7d0f","execution":{"iopub.status.busy":"2022-01-17T06:42:20.027497Z","iopub.execute_input":"2022-01-17T06:42:20.027848Z","iopub.status.idle":"2022-01-17T06:42:20.096332Z","shell.execute_reply.started":"2022-01-17T06:42:20.027792Z","shell.execute_reply":"2022-01-17T06:42:20.095463Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Then our model looks almost identical to the first model, except with a different embedding layer","metadata":{"_uuid":"1ae26e909f2b524d654aa50b9d8c7127630da00c"}},{"cell_type":"code","source":"inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n\n# note the `trainable=False`, later we will make this layer trainable\nembedding_layer_2 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=False)(inputs_2)\n\nreshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n\nconv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\nconv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\nconv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n\nmaxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\nmaxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\nmaxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n\nconcatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\nflatten_2 = Flatten()(concatenated_tensor_2)\n\ndropout_2 = Dropout(0.5)(flatten_2)\noutput_2 = Dense(units=5, activation='softmax')(dropout_2)","metadata":{"_uuid":"e63faa267307e89d72ecc591de05b6ae036d3c50","execution":{"iopub.status.busy":"2022-01-17T06:42:23.134046Z","iopub.execute_input":"2022-01-17T06:42:23.134651Z","iopub.status.idle":"2022-01-17T06:42:23.471660Z","shell.execute_reply.started":"2022-01-17T06:42:23.134596Z","shell.execute_reply":"2022-01-17T06:42:23.470553Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model_2 = Model(inputs=inputs_2, outputs=output_2)\nmodel_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_2.summary())","metadata":{"_uuid":"33cd071c9b72e11641ef8c53fc016ef2ddc42aa4","execution":{"iopub.status.busy":"2022-01-17T06:42:26.849237Z","iopub.execute_input":"2022-01-17T06:42:26.849602Z","iopub.status.idle":"2022-01-17T06:42:26.911861Z","shell.execute_reply.started":"2022-01-17T06:42:26.849542Z","shell.execute_reply":"2022-01-17T06:42:26.910892Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_2 = model_2.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"0ac83b5caf0d64aac8aea8debac7d731d60cc4ca","execution":{"iopub.status.busy":"2022-01-17T06:42:30.297060Z","iopub.execute_input":"2022-01-17T06:42:30.297402Z","iopub.status.idle":"2022-01-17T07:03:27.363949Z","shell.execute_reply.started":"2022-01-17T06:42:30.297352Z","shell.execute_reply":"2022-01-17T07:03:27.363058Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_2.history['acc'])\nplt.plot(history_2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"30e5f8be7138bfb5646353d4456bdeed06a07d0e","execution":{"iopub.status.busy":"2022-01-17T07:03:49.071572Z","iopub.execute_input":"2022-01-17T07:03:49.072297Z","iopub.status.idle":"2022-01-17T07:03:49.590554Z","shell.execute_reply.started":"2022-01-17T07:03:49.072222Z","shell.execute_reply":"2022-01-17T07:03:49.589419Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"y_hat_2 = model_2.predict(X_test)","metadata":{"_uuid":"3ae4f039d0b614086acabc2cca68a5ce102bd924","execution":{"iopub.status.busy":"2022-01-17T07:03:55.554889Z","iopub.execute_input":"2022-01-17T07:03:55.555280Z","iopub.status.idle":"2022-01-17T07:03:57.444502Z","shell.execute_reply.started":"2022-01-17T07:03:55.555214Z","shell.execute_reply":"2022-01-17T07:03:57.443671Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))","metadata":{"_uuid":"98f136828c411234d9a3175feb9ebf0669739527","execution":{"iopub.status.busy":"2022-01-17T07:03:59.595177Z","iopub.execute_input":"2022-01-17T07:03:59.595504Z","iopub.status.idle":"2022-01-17T07:03:59.625178Z","shell.execute_reply.started":"2022-01-17T07:03:59.595466Z","shell.execute_reply":"2022-01-17T07:03:59.624310Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))","metadata":{"_uuid":"607f795a1df687b27d109f5c72d1f8be34e5aa01","execution":{"iopub.status.busy":"2022-01-17T07:04:02.677609Z","iopub.execute_input":"2022-01-17T07:04:02.678099Z","iopub.status.idle":"2022-01-17T07:04:02.712168Z","shell.execute_reply.started":"2022-01-17T07:04:02.678049Z","shell.execute_reply":"2022-01-17T07:04:02.711197Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"So you can see this model doesn't do that much better (maybe a bit worse depending how this run went) than the random initialization. This agrees with what Kim found.","metadata":{"_uuid":"d5763c587ddb1db0624b2753d31c5e05d8407a20"}},{"cell_type":"markdown","source":"# Model 3: w2v with trainable embeddings\n\nFor this model we're going to try the same model again, but this time make the embeddings trainable. That means if during training the model decides on a better embedding for a word then it'll update it\n\nThis has the added benifit of updating the words which were randomly assigned a vector (because they weren't in the w2v model)","metadata":{"_uuid":"5268ddd4d449ce7540b7ef7269427e921731e3e8"}},{"cell_type":"code","source":"inputs_3 = Input(shape=(sequence_length,), dtype='int32')\nembedding_layer_3 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=True)(inputs_3)\n\nreshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n\n# note the relu activation\nconv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\nconv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\nconv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n\nmaxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\nmaxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\nmaxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n\nconcatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\nflatten_3 = Flatten()(concatenated_tensor_3)\n\ndropout_3 = Dropout(0.5)(flatten_3)\noutput_3 = Dense(units=5, activation='softmax')(dropout_3)","metadata":{"_uuid":"c36ceefeeaa74f76bae42c5ac4d39691d6caa219","execution":{"iopub.status.busy":"2022-01-17T07:04:06.267452Z","iopub.execute_input":"2022-01-17T07:04:06.267785Z","iopub.status.idle":"2022-01-17T07:04:06.445357Z","shell.execute_reply.started":"2022-01-17T07:04:06.267737Z","shell.execute_reply":"2022-01-17T07:04:06.444292Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model_3 = Model(inputs=inputs_3, outputs=output_3)\nmodel_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_3.summary())","metadata":{"_uuid":"4560d13f8279ab60e5032ae79b3e30430220109e","execution":{"iopub.status.busy":"2022-01-17T07:04:16.395192Z","iopub.execute_input":"2022-01-17T07:04:16.395544Z","iopub.status.idle":"2022-01-17T07:04:16.454278Z","shell.execute_reply.started":"2022-01-17T07:04:16.395495Z","shell.execute_reply":"2022-01-17T07:04:16.453100Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_3 = model_3.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"db3b163e78d48fbaff9de0847548aad06b2bc90f","execution":{"iopub.status.busy":"2022-01-17T07:04:20.395458Z","iopub.execute_input":"2022-01-17T07:04:20.395800Z","iopub.status.idle":"2022-01-17T07:49:47.902638Z","shell.execute_reply.started":"2022-01-17T07:04:20.395760Z","shell.execute_reply":"2022-01-17T07:49:47.901379Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_3.history['acc'])\nplt.plot(history_3.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_3.history['loss'])\nplt.plot(history_3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"0180a0bb915f291cf9bbd2f5d3500cd1a5e7cd81","execution":{"iopub.status.busy":"2022-01-17T07:50:09.227616Z","iopub.execute_input":"2022-01-17T07:50:09.228169Z","iopub.status.idle":"2022-01-17T07:50:09.803447Z","shell.execute_reply.started":"2022-01-17T07:50:09.228061Z","shell.execute_reply":"2022-01-17T07:50:09.798776Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"y_hat_3 = model_3.predict(X_test)","metadata":{"_uuid":"581bc847d6b10330d76eacfc22fdafe66f961fb2","execution":{"iopub.status.busy":"2022-01-17T07:50:13.171010Z","iopub.execute_input":"2022-01-17T07:50:13.171746Z","iopub.status.idle":"2022-01-17T07:50:15.232035Z","shell.execute_reply.started":"2022-01-17T07:50:13.171672Z","shell.execute_reply":"2022-01-17T07:50:15.231134Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))","metadata":{"_uuid":"e54f6dd1fd37ec8a8fdf8ec75f814c4c6e54261c","execution":{"iopub.status.busy":"2022-01-17T07:50:16.912726Z","iopub.execute_input":"2022-01-17T07:50:16.913128Z","iopub.status.idle":"2022-01-17T07:50:16.947359Z","shell.execute_reply.started":"2022-01-17T07:50:16.913057Z","shell.execute_reply":"2022-01-17T07:50:16.946200Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))","metadata":{"_uuid":"b9762538c6f57badf6062e78a040df99282f6d08","execution":{"iopub.status.busy":"2022-01-17T07:50:22.017179Z","iopub.execute_input":"2022-01-17T07:50:22.017555Z","iopub.status.idle":"2022-01-17T07:50:22.059873Z","shell.execute_reply.started":"2022-01-17T07:50:22.017499Z","shell.execute_reply":"2022-01-17T07:50:22.058527Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"As you can see this performs much better than that random vector and static w2v models - by 10% which is a massive improvement!","metadata":{"_uuid":"c32a65a441053d5087f691562d51700d6e1b6266"}},{"cell_type":"markdown","source":"# Model 4: Again, but with binary classification (similar to SST2)\n\nKim tried his models against a varity of datasets. Two of these datasets are closely related to one another (and similar to our dataset).\n\nSST1 is a semantic dataset release by Stanford. It has 5 categories of sentiment (like our data). SST2 is the same dataset, but with very negative and slightly negative merged into one class and slightly positive and positive also merged (neutral is dropped). This gives us a binary classification we can replicate with our data","metadata":{"_uuid":"e1ce30795b88c489b15e97624d1b82afda8322bb"}},{"cell_type":"code","source":"# SST2 is the same as SST1, but with neutrals removed and binary labels\nsst2_data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)\n\ndef merge_sentiments(x):\n    if x == 0 or x == 1:\n        return 0\n    else:\n        return 1\n\nsst2_data['Sentiment'] = sst2_data['Sentiment'].apply(merge_sentiments)\n\nsst2_tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\nsst2_tokenizer.fit_on_texts(sst2_data['Phrase'].values)\n\nsst2_X = sst2_tokenizer.texts_to_sequences(sst2_data['Phrase'].values)\nsst2_X = pad_sequences(sst2_X, sequence_length)\n\nsst2_y = sst2_data['Sentiment'].values\n\nsst2_X_train, sst2_X_test, sst2_y_train, sst2_y_test = train_test_split(sst2_X, sst2_y, test_size=0.1)","metadata":{"_uuid":"cac9094cfd84393e0426b5955fe9f3ba52b5aef7","execution":{"iopub.status.busy":"2022-01-17T07:50:25.868290Z","iopub.execute_input":"2022-01-17T07:50:25.868676Z","iopub.status.idle":"2022-01-17T07:50:27.461121Z","shell.execute_reply.started":"2022-01-17T07:50:25.868613Z","shell.execute_reply":"2022-01-17T07:50:27.460217Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Now lets try to train our model again with this dataset","metadata":{"_uuid":"30e101528850aab3a8565cf99ef38ee0dd770531"}},{"cell_type":"code","source":"inputs_4 = Input(shape=(sequence_length,), dtype='int32')\nembedding_layer_4 = Embedding(num_words,\n                            embedding_dim,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=sequence_length,\n                            trainable=True)(inputs_4)\n\nreshape_4 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_4)\n\nconv_0_4 = Conv2D(num_filters, kernel_size=(3, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\nconv_1_4 = Conv2D(num_filters, kernel_size=(4, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\nconv_2_4 = Conv2D(num_filters, kernel_size=(5, embedding_dim), padding='valid', kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_4)\n\nmaxpool_0_4 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_4)\nmaxpool_1_4 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_4)\nmaxpool_2_4 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_4)\n\nconcatenated_tensor_4 = Concatenate(axis=1)([maxpool_0_4, maxpool_1_4, maxpool_2_4])\nflatten_4 = Flatten()(concatenated_tensor_4)\n\ndropout_4 = Dropout(0.5)(flatten_4)\n# note the different activation\noutput_4 = Dense(units=1, activation='sigmoid')(dropout_4)","metadata":{"_uuid":"1b92d6ec454dfe7e60d96367655bbb060802bae2","execution":{"iopub.status.busy":"2022-01-17T07:50:30.751505Z","iopub.execute_input":"2022-01-17T07:50:30.751837Z","iopub.status.idle":"2022-01-17T07:50:30.935530Z","shell.execute_reply.started":"2022-01-17T07:50:30.751789Z","shell.execute_reply":"2022-01-17T07:50:30.934478Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model_4 = Model(inputs=inputs_4, outputs=output_4)\n\n# note we're using binary_crossentropy here instead of categorical\nmodel_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_4.summary())","metadata":{"_uuid":"cf714659cbc8f7daa9bfd5a80a04c735e1cdffa2","execution":{"iopub.status.busy":"2022-01-17T07:50:37.018856Z","iopub.execute_input":"2022-01-17T07:50:37.019237Z","iopub.status.idle":"2022-01-17T07:50:37.090638Z","shell.execute_reply.started":"2022-01-17T07:50:37.019187Z","shell.execute_reply":"2022-01-17T07:50:37.089517Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory_4 = model_4.fit(sst2_X_train, sst2_y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)","metadata":{"_uuid":"e9626b4a28e974d1844d5182180c07acbbba633f","execution":{"iopub.status.busy":"2022-01-17T07:50:40.954797Z","iopub.execute_input":"2022-01-17T07:50:40.955182Z","iopub.status.idle":"2022-01-17T08:27:24.037617Z","shell.execute_reply.started":"2022-01-17T07:50:40.955121Z","shell.execute_reply":"2022-01-17T08:27:24.036482Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_4.history['acc'])\nplt.plot(history_4.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history_4.history['loss'])\nplt.plot(history_4.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"_uuid":"79b1d3a3ebc123e078c6b1240bf96c72c050ae7e","execution":{"iopub.status.busy":"2022-01-17T08:28:26.072117Z","iopub.execute_input":"2022-01-17T08:28:26.072611Z","iopub.status.idle":"2022-01-17T08:28:26.592549Z","shell.execute_reply.started":"2022-01-17T08:28:26.072513Z","shell.execute_reply":"2022-01-17T08:28:26.591364Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"y_hat_4 = model_4.predict(sst2_X_test)\n\naccuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))","metadata":{"_uuid":"e07af5424e49a3a2cf1111747a8799454b2fb20f","execution":{"iopub.status.busy":"2022-01-17T08:28:31.352527Z","iopub.execute_input":"2022-01-17T08:28:31.353133Z","iopub.status.idle":"2022-01-17T08:28:35.150224Z","shell.execute_reply.started":"2022-01-17T08:28:31.353075Z","shell.execute_reply":"2022-01-17T08:28:35.149233Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))","metadata":{"_uuid":"9f11553056a74d334c760c0d372e57cac5b257d9","execution":{"iopub.status.busy":"2022-01-17T08:28:37.419218Z","iopub.execute_input":"2022-01-17T08:28:37.420133Z","iopub.status.idle":"2022-01-17T08:28:37.464570Z","shell.execute_reply.started":"2022-01-17T08:28:37.420049Z","shell.execute_reply":"2022-01-17T08:28:37.463365Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nHere's the final results","metadata":{"_uuid":"00cd755da0c3e70ad3bbb71dc061997052307c15"}},{"cell_type":"code","source":"print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\nprint(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\nprint(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))\nprint(\"Binary trainable : \" + str(accuracy_score(sst2_y_test, list(map(lambda v: v > 0.5, y_hat_4)))))","metadata":{"_uuid":"b609a6f9df9d26c1cde894f98da4625b895461ea","execution":{"iopub.status.busy":"2022-01-17T08:28:40.974680Z","iopub.execute_input":"2022-01-17T08:28:40.975173Z","iopub.status.idle":"2022-01-17T08:28:41.106891Z","shell.execute_reply.started":"2022-01-17T08:28:40.975126Z","shell.execute_reply":"2022-01-17T08:28:41.105837Z"},"trusted":true},"execution_count":48,"outputs":[]}]}